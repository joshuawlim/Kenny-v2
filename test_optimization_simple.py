#!/usr/bin/env python3
"""
Simple standalone test for Phase 2 optimization concepts
Tests the core optimization logic without complex imports
"""

import re
import time
from enum import Enum
from typing import Dict, Any, Tuple

class QueryComplexity(Enum):
    """Query complexity levels"""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    AMBIGUOUS = "ambiguous"

class SimpleModelRouter:
    """Simplified model router for testing optimization concepts"""
    
    def __init__(self):
        # Model performance characteristics (from benchmarking)
        self.models = {
            "llama3.2:3b-instruct": {
                "avg_response_time": 2.1,  # Fastest
                "accuracy_score": 0.85,
                "max_complexity": QueryComplexity.MODERATE
            },
            "qwen2.5:3b-instruct": {
                "avg_response_time": 2.8,  # Balanced
                "accuracy_score": 0.88,
                "max_complexity": QueryComplexity.COMPLEX
            },
            "qwen3:8b": {
                "avg_response_time": 12.5,  # Slowest but most accurate
                "accuracy_score": 0.92,
                "max_complexity": QueryComplexity.COMPLEX
            }
        }
    
    def analyze_query_complexity(self, query: str) -> Tuple[QueryComplexity, float, str]:
        """Analyze query complexity using heuristics"""
        query_lower = query.lower()
        
        # Length analysis
        words = len(query.split())
        sentences = len(re.split(r'[.!?]+', query.strip()))
        
        # Pattern analysis
        simple_patterns = [
            r'\b(show|get|find|check)\s+my\s+(email|calendar|contacts?)\b',
            r'\b(search|list)\s+(email|message|contact)s?\b'
        ]
        
        complex_patterns = [
            r'\b(find|search)\s+.*\s+(and|then)\s+(schedule|create|send)\b',
            r'\b(compare|analyze|summarize)\b'
        ]
        
        multi_step_patterns = [
            r'\b(then|after|next|also|additionally)\b',
            r'\b(and\s+then|and\s+also)\b'
        ]
        
        ambiguous_patterns = [
            r'\b(something|anything|stuff|thing)\b',
            r'\b(maybe|perhaps|possibly|might)\b',
            r'\b(help|assist)\s+me\s+with\b'
        ]
        
        # Multi-agent complexity
        agent_domains = {
            "email": ["email", "mail", "inbox"],
            "calendar": ["calendar", "schedule", "meeting", "appointment"],
            "contacts": ["contact", "person", "phone", "address"]
        }
        
        domains_mentioned = 0
        for domain, keywords in agent_domains.items():
            if any(keyword in query_lower for keyword in keywords):
                domains_mentioned += 1
        
        # Calculate complexity score
        complexity_score = 0.2  # Base score
        reasoning_parts = []
        
        # Length factor
        if words <= 5:
            complexity_score += 0.0
            reasoning_parts.append("short query")
        elif words <= 15:
            complexity_score += 0.2
            reasoning_parts.append("medium length")
        else:
            complexity_score += 0.4
            reasoning_parts.append("long query")
        
        # Pattern matching
        for pattern in simple_patterns:
            if re.search(pattern, query_lower):
                complexity_score = max(0.2, complexity_score)
                reasoning_parts.append("simple action pattern")
                break
        
        for pattern in complex_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 0.3
                reasoning_parts.append("complex action pattern")
                break
        
        for pattern in multi_step_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 0.2
                reasoning_parts.append("multi-step request")
                break
        
        for pattern in ambiguous_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 0.3
                reasoning_parts.append("ambiguous language")
                break
        
        # Multi-agent factor
        if domains_mentioned >= 2:
            complexity_score += 0.3
            reasoning_parts.append(f"{domains_mentioned} domains mentioned")
        
        # Determine complexity level
        if complexity_score <= 0.3:
            complexity = QueryComplexity.SIMPLE
        elif complexity_score <= 0.5:
            complexity = QueryComplexity.MODERATE
        elif complexity_score <= 0.7:
            complexity = QueryComplexity.COMPLEX
        else:
            complexity = QueryComplexity.AMBIGUOUS
        
        # Confidence based on how decisive the patterns are
        confidence = min(0.95, 0.6 + (complexity_score * 0.5))
        reasoning = f"{complexity.value}: {', '.join(reasoning_parts) if reasoning_parts else 'default classification'}"
        
        return complexity, confidence, reasoning
    
    def select_optimal_model(self, complexity: QueryComplexity, current_avg_time: float = 3.0) -> Tuple[str, str]:
        """Select optimal model based on complexity and performance constraints"""
        
        # Performance-first for simple queries
        if complexity == QueryComplexity.SIMPLE:
            return "llama3.2:3b-instruct", "Simple query -> fastest model"
        
        # Balanced approach for moderate complexity
        elif complexity == QueryComplexity.MODERATE:
            if current_avg_time > 5.0:
                # Performance degraded, prefer speed
                return "llama3.2:3b-instruct", "Performance degraded -> fast model"
            else:
                return "qwen2.5:3b-instruct", "Moderate complexity -> balanced model"
        
        # Complex/ambiguous queries
        else:
            if current_avg_time <= 3.0:  # Performance is good
                return "qwen2.5:3b-instruct", "Complex query with good performance -> balanced model"
            else:
                return "qwen2.5:3b-instruct", "Complex query with performance constraints -> balanced fallback"
    
    def route_query(self, query: str, current_performance: Dict[str, float] = None) -> Dict[str, Any]:
        """Route query to optimal model"""
        if current_performance is None:
            current_performance = {"avg_response_time": 3.0}
        
        # Analyze complexity
        complexity, confidence, reasoning = self.analyze_query_complexity(query)
        
        # Select model
        selected_model, selection_reason = self.select_optimal_model(
            complexity, current_performance.get("avg_response_time", 3.0)
        )
        
        # Get performance estimate
        estimated_time = self.models[selected_model]["avg_response_time"]
        
        return {
            "selected_model": selected_model,
            "complexity": complexity,
            "confidence": confidence,
            "complexity_reasoning": reasoning,
            "selection_reasoning": selection_reason,
            "estimated_response_time": estimated_time,
            "accuracy_estimate": self.models[selected_model]["accuracy_score"]
        }

def test_query_complexity():
    """Test query complexity analysis"""
    print("üß† Testing Query Complexity Analysis")
    print("="*40)
    
    router = SimpleModelRouter()
    
    test_cases = [
        ("check email", QueryComplexity.SIMPLE),
        ("show my calendar", QueryComplexity.SIMPLE),
        ("find emails from Sarah about project", QueryComplexity.MODERATE),
        ("search messages and then schedule meeting", QueryComplexity.COMPLEX),
        ("analyze my calendar and optimize my schedule", QueryComplexity.COMPLEX),
        ("help me with something about meetings", QueryComplexity.AMBIGUOUS),
        ("find the thing from yesterday maybe", QueryComplexity.AMBIGUOUS)
    ]
    
    results = []
    for query, expected in test_cases:
        complexity, confidence, reasoning = router.analyze_query_complexity(query)
        success = complexity == expected
        
        results.append({
            "query": query,
            "expected": expected.value,
            "actual": complexity.value,
            "confidence": confidence,
            "success": success
        })
        
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} '{query}'")
        print(f"   Expected: {expected.value}, Got: {complexity.value} (confidence: {confidence:.2f})")
        print(f"   Reasoning: {reasoning}")
        print()
    
    accuracy = sum(r["success"] for r in results) / len(results)
    print(f"üéØ Accuracy: {accuracy:.1%}\n")
    
    return results

def test_model_selection():
    """Test model selection logic"""
    print("‚ö° Testing Model Selection Logic")
    print("="*40)
    
    router = SimpleModelRouter()
    
    test_scenarios = [
        # Good performance scenarios
        {
            "query": "check email",
            "performance": {"avg_response_time": 2.0},
            "expected_model": "llama3.2:3b-instruct"
        },
        {
            "query": "find emails from Sarah about project",
            "performance": {"avg_response_time": 2.5},
            "expected_model": "qwen2.5:3b-instruct"
        },
        # Degraded performance scenarios
        {
            "query": "find emails from Sarah and schedule meeting",
            "performance": {"avg_response_time": 6.0},
            "expected_model": "qwen2.5:3b-instruct"  # Still balanced for complex
        },
        {
            "query": "search messages containing meeting details",
            "performance": {"avg_response_time": 6.0},
            "expected_model": "llama3.2:3b-instruct"  # Fast for moderate when degraded
        }
    ]
    
    results = []
    for scenario in test_scenarios:
        routing_result = router.route_query(scenario["query"], scenario["performance"])
        selected_model = routing_result["selected_model"]
        expected_model = scenario["expected_model"]
        
        success = selected_model == expected_model
        results.append({
            "query": scenario["query"],
            "expected_model": expected_model,
            "selected_model": selected_model,
            "success": success,
            "reasoning": routing_result["selection_reasoning"]
        })
        
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} '{scenario['query']}'")
        print(f"   Performance: {scenario['performance']['avg_response_time']}s avg")
        print(f"   Expected: {expected_model}, Got: {selected_model}")
        print(f"   Reasoning: {routing_result['selection_reasoning']}")
        print(f"   Complexity: {routing_result['complexity'].value} (confidence: {routing_result['confidence']:.2f})")
        print()
    
    accuracy = sum(r["success"] for r in results) / len(results)
    print(f"üéØ Selection Accuracy: {accuracy:.1%}\n")
    
    return results

def test_performance_improvement():
    """Test performance improvement estimates"""
    print("üìà Testing Performance Improvement")
    print("="*40)
    
    router = SimpleModelRouter()
    
    # Simulate various queries and their routing
    test_queries = [
        "check my email",
        "show calendar events", 
        "find contacts named John",
        "search emails from Sarah about project updates",
        "find messages containing meeting and schedule follow-up",
        "analyze my calendar and suggest optimal meeting times",
        "help me find something about quarterly reviews"
    ]
    
    baseline_time = 12.5  # Current Qwen3:8b baseline
    total_optimized_time = 0
    results = []
    
    print("Query Routing Results:")
    for query in test_queries:
        routing_result = router.route_query(query)
        selected_model = routing_result["selected_model"]
        estimated_time = routing_result["estimated_response_time"]
        complexity = routing_result["complexity"]
        
        total_optimized_time += estimated_time
        improvement = ((baseline_time - estimated_time) / baseline_time) * 100
        
        results.append({
            "query": query,
            "selected_model": selected_model,
            "estimated_time": estimated_time,
            "improvement_percent": improvement,
            "complexity": complexity.value
        })
        
        print(f"  '{query}'")
        print(f"    Model: {selected_model}")
        print(f"    Time: {estimated_time:.1f}s (baseline: {baseline_time:.1f}s)")
        print(f"    Improvement: {improvement:.1f}%")
        print(f"    Complexity: {complexity.value}")
        print()
    
    # Calculate overall improvement
    avg_optimized_time = total_optimized_time / len(test_queries)
    avg_baseline_time = baseline_time  # All queries would use Qwen3:8b in baseline
    
    improvement_factor = avg_baseline_time / avg_optimized_time
    improvement_percent = ((avg_baseline_time - avg_optimized_time) / avg_baseline_time) * 100
    target_met = avg_optimized_time <= 5.0
    
    print(f"üìâ Performance Summary:")
    print(f"  Baseline Average: {avg_baseline_time:.1f}s (Qwen3:8b for all)")
    print(f"  Optimized Average: {avg_optimized_time:.1f}s")
    print(f"  Improvement Factor: {improvement_factor:.1f}x")
    print(f"  Speed Improvement: {improvement_percent:.1f}%")
    print(f"  Target (<5s): {'‚úÖ MET' if target_met else '‚ùå NOT MET'}")
    
    # Target assessment
    target_improvement = 8.0  # 8-10x target
    target_achieved = improvement_factor >= target_improvement
    
    print(f"\nüéØ Target Assessment:")
    print(f"  Target Improvement: {target_improvement}x")
    print(f"  Actual Improvement: {improvement_factor:.1f}x")
    print(f"  Target Status: {'‚úÖ ACHIEVED' if target_achieved else '‚ùå PARTIAL' if improvement_factor >= 5.0 else '‚ùå NOT ACHIEVED'}")
    
    return {
        "results": results,
        "avg_optimized_time": avg_optimized_time,
        "improvement_factor": improvement_factor,
        "improvement_percent": improvement_percent,
        "target_met": target_met,
        "target_achieved": target_achieved
    }

def main():
    """Run all tests"""
    print("üöÄ Phase 2: Coordinator Model Optimization - Concept Test")
    print("="*60)
    print()
    
    # Test 1: Query complexity analysis
    complexity_results = test_query_complexity()
    
    # Test 2: Model selection logic
    selection_results = test_model_selection()
    
    # Test 3: Performance improvement estimation
    performance_results = test_performance_improvement()
    
    # Final assessment
    print("\n" + "="*60)
    print("üéâ FINAL ASSESSMENT")
    print("="*60)
    
    complexity_accuracy = sum(r["success"] for r in complexity_results) / len(complexity_results)
    selection_accuracy = sum(r["success"] for r in selection_results) / len(selection_results)
    target_met = performance_results["target_met"]
    target_achieved = performance_results["target_achieved"]
    
    print(f"\nüìä Results Summary:")
    print(f"  Complexity Analysis Accuracy: {complexity_accuracy:.1%}")
    print(f"  Model Selection Accuracy: {selection_accuracy:.1%}")
    print(f"  Response Time Target (<5s): {'‚úÖ MET' if target_met else '‚ùå NOT MET'}")
    print(f"  Improvement Target (8x): {'‚úÖ ACHIEVED' if target_achieved else '‚ùå NOT ACHIEVED'}")
    print(f"  Actual Improvement: {performance_results['improvement_factor']:.1f}x")
    print(f"  Average Response Time: {performance_results['avg_optimized_time']:.1f}s")
    
    overall_success = (
        complexity_accuracy >= 0.7 and
        selection_accuracy >= 0.5 and
        target_met
    )
    
    print(f"\nüéØ Overall Assessment: {'‚úÖ SUCCESS' if overall_success else '‚ùå NEEDS IMPROVEMENT'}")
    
    if overall_success:
        print("\n‚úÖ Phase 2 optimization concepts are working effectively!")
        print(f"  - {performance_results['improvement_factor']:.1f}x faster than baseline")
        print(f"  - {performance_results['improvement_percent']:.1f}% speed improvement")
        print(f"  - Consistent sub-5 second response times")
        print("  - Intelligent model routing based on query complexity")
        print("\nüöÄ Ready for production implementation!")
    else:
        print("\n‚ùå Some optimization concepts need refinement:")
        if complexity_accuracy < 0.7:
            print("  - Improve complexity analysis accuracy")
        if selection_accuracy < 0.5:
            print("  - Refine model selection logic")
        if not target_met:
            print("  - Optimize model performance or add caching")
    
    return {
        "complexity_accuracy": complexity_accuracy,
        "selection_accuracy": selection_accuracy,
        "performance_improvement": performance_results["improvement_factor"],
        "avg_response_time": performance_results["avg_optimized_time"],
        "target_met": target_met,
        "target_achieved": target_achieved,
        "overall_success": overall_success
    }

if __name__ == "__main__":
    main()